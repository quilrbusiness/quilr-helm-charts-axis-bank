INSERT INTO public.compliance_master_nist (id,classification,control_id,control_name,created_at,description,evidence_collection_method,example_artifact,format,framework_category,mitigation_if_failed,points_assigned,responsible_role,row_id,score_if_passed,scoring_method,software_system_to_integrate,source_reference,target_ai_asset,updated_at) VALUES
	 ('d88014b4-dfb3-4395-a8f4-df797a1cd865'::uuid,'Operational','GOVERN 1.1','AI Legal Requirements','2025-12-03 19:12:27.062326','Legal and regulatory requirements involving AI are understood, managed, and documented.','Manual Upload','Policy Document','PDF','GOVERN (Governance)','Conduct a legal review to identify AI-related obligations (privacy, discrimination, etc.); document how each requirement is addressed and maintain records of compliance checks.',5,'AI Risk Officer',1,5,'Full points if all applicable AI laws/regs are identified and compliance measures documented.','ServiceNow','NIST AI RMF 1.0 – Govern 1.1','Organization (Compliance)','2025-12-03 19:12:27.062326'),
	 ('d162d44a-525b-40ad-b7da-f8b7658e6afe'::uuid,'Operational','GOVERN 1.2','Trustworthy AI Integration','2025-12-03 19:12:27.062326','The characteristics of trustworthy AI (e.g. valid, reliable, safe, secure, explainable, fair) are integrated into organizational policies, processes, and practices.','Automated Workflow','Training Records','XLSX','GOVERN (Governance)','Update corporate policies (e.g. IT governance, product development guidelines) to incorporate AI trustworthiness principles (safety, fairness, etc.); train staff to apply these in practice.',5,'Compliance Manager',2,5,'Full points if the organization’s policies explicitly include all relevant AI trustworthiness criteria.','Jira','NIST AI RMF 1.0 – Govern 1.2','Organization (Policies & Standards)','2025-12-03 19:12:27.062326'),
	 ('67ddf27e-8001-40e1-ac34-845b8911ded5'::uuid,'Operational','GOVERN 1.3','Risk Management Level Set','2025-12-03 19:12:27.062326','Processes, procedures, and practices are in place to determine the needed level of AI risk management activities based on the organization’s risk tolerance.','Third-Party Integration','Risk Register','CSV','GOVERN (Governance)','Develop a risk tiering system for AI projects (e.g. classify AI systems as low/medium/high risk) and define which risk management steps are required at each tier; ensure teams apply this consistently.',5,'Data Protection Officer',3,5,'Full points if there’s a documented methodology to scale AI risk efforts (e.g. more controls for higher-risk systems) aligned with risk appetite.','Confluence','NIST AI RMF 1.0 – Govern 1.3','Organization (Risk Management)','2025-12-03 19:12:27.062326'),
	 ('af7e2686-807a-4d42-9d79-ace2e5dc4c28'::uuid,'Operational','GOVERN 1.4','Transparent Risk Process','2025-12-03 19:12:27.062326','The AI risk management process and outcomes are established through transparent policies, procedures, and controls based on organizational risk priorities.','Manual Entry','System Design Spec','DOCX','GOVERN (Governance)','Document the AI risk management framework steps (e.g. in a risk management policy or standard); maintain an AI risk register and share summaries of risk assessments and mitigations with relevant oversight bodies (e.g. risk committee).',5,'Product Manager',4,5,'Full points if the AI RMF process (and results like risk registers) is well-documented and visible to stakeholders.','OneTrust','NIST AI RMF 1.0 – Govern 1.4','Organization (Risk Management)','2025-12-03 19:12:27.062326'),
	 ('caae8c36-90f3-4a4c-be0f-91abdcda6de0'::uuid,'Operational','GOVERN 1.5','Risk Process Monitoring & Review','2025-12-03 19:12:27.062326','Ongoing monitoring and periodic review of the AI risk management process and its outcomes are planned, with roles and responsibilities clearly defined (including review frequency).','System Sync','Audit Trail','LOG','GOVERN (Governance)','Establish a schedule (e.g. quarterly) for management to review AI risk reports and the performance of risk controls; assign clear ownership (e.g. Chief Risk Officer or AI Governance Board) for ensuring these reviews happen and improvements are implemented.',5,'Audit Lead',5,5,'Full points if a governance body (e.g. AI risk committee) regularly reviews the effectiveness of AI risk management (partial if reviews are infrequent or informal).','RiskLens','NIST AI RMF 1.0 – Govern 1.5','Organization (Risk Oversight)','2025-12-03 19:12:27.062326'),
	 ('f932dabe-027f-4003-b08a-102deab9f541'::uuid,'Operational/Technical','GOVERN 1.6','AI System Inventory','2025-12-03 19:12:27.062326','Mechanisms are in place to inventory AI systems, and they are resourced according to organizational risk priorities.','Manual Upload','Stakeholder Feedback Log','PDF','GOVERN (Governance)','Maintain a living inventory of all AI systems (including details like purpose, algorithm, data, owner, risk level); allocate resources (funding, personnel) preferentially to higher-risk systems as identified in the inventory.',5,'AI Risk Officer',6,5,'Full points if a current inventory of all AI systems (with risk ratings and resource allocations) exists (partial if inventory incomplete or not tied to risk).','ServiceNow','NIST AI RMF 1.0 – Govern 1.6','Organization (Asset Management)','2025-12-03 19:12:27.062326'),
	 ('7aaec2d7-6374-4992-8a09-916cb525057c'::uuid,'Operational','GOVERN 1.7','Safe Decommissioning','2025-12-03 19:12:27.062326','Processes and procedures are in place for decommissioning and phasing out AI systems safely, without increasing risks or reducing trust.','Automated Workflow','Incident Report','XLSX','GOVERN (Governance)','Develop a decommissioning checklist for AI systems (covering data archival or deletion, model disposal, notification to users, continuity plans if needed) and require sign-off from risk management before an AI system is turned off.',5,'Compliance Manager',7,5,'Full points if a formal retirement plan (ensuring data/model disposal, stakeholder notice, fallback processes) is required for AI systems (partial if systems retired ad-hoc).','Jira','NIST AI RMF 1.0 – Govern 1.7','Organization (Lifecycle Management)','2025-12-03 19:12:27.062326'),
	 ('ef301d45-6fd7-4d0e-abf3-b276b7f9ce66'::uuid,'Operational','GOVERN 2.1','Roles & Communications for AI Risk','2025-12-03 19:12:27.062326','Roles, responsibilities, and lines of communication related to mapping, measuring, and managing AI risks are documented and clear to individuals and teams across the organization.','Third-Party Integration','Change Request Form','CSV','GOVERN (Governance)','Define and document the AI risk governance structure (e.g. assign risk officers or liaisons in each AI product team, appoint an AI risk lead); communicate these roles via training or internal websites so everyone knows points of contact for AI risk issues.',5,'Data Protection Officer',8,5,'Full points if an org chart or RACI for AI risk management exists (covering who does risk identification, mitigation, etc.) and is communicated (partial if roles defined but not widely known).','Confluence','NIST AI RMF 1.0 – Govern 2.1','Organization (Workforce)','2025-12-03 19:12:27.062326'),
	 ('15bb2a6b-5708-4940-ae26-718c9224a269'::uuid,'Operational','GOVERN 2.2','AI Risk Management Training','2025-12-03 19:12:27.062326','The organization’s personnel and partners receive AI risk management training to perform their duties consistent with policies, procedures, and agreements.','Manual Entry','Executive Sign-off Memo','DOCX','GOVERN (Governance)','Develop and deliver regular training modules on AI risk management topics (e.g. bias mitigation, security of ML models, regulatory compliance) for employees involved in AI, and extend awareness sessions or guidelines to key partners/vendors.',5,'Product Manager',9,5,'Full points if a training program on AI risks (ethics, security, etc.) is implemented for relevant staff and partners (partial if only informal awareness or limited to some teams).','OneTrust','NIST AI RMF 1.0 – Govern 2.2','Organization (Workforce)','2025-12-03 19:12:27.062326'),
	 ('c7cff34f-aa57-4a75-a7f1-d37f9f9f6b84'::uuid,'Operational','GOVERN 2.3','Executive Accountability','2025-12-03 19:12:27.062326','Executive leadership takes responsibility for decisions about risks associated with AI system development and deployment.','System Sync','Data Lineage Report','LOG','GOVERN (Governance)','Require that significant AI initiatives and their risk assessments are reviewed and approved by senior leadership (e.g. CEO, CIO, Chief AI Officer); include AI risks in enterprise risk discussions at the executive/board level to ensure accountability.',5,'Audit Lead',10,5,'Full points if a senior executive (or committee) explicitly signs off on AI risk acceptances or key decisions (partial if risk decisions are pushed at lower levels without oversight).','RiskLens','NIST AI RMF 1.0 – Govern 2.3','Organization (Leadership)','2025-12-03 19:12:27.062326');
INSERT INTO public.compliance_master_nist (id,classification,control_id,control_name,created_at,description,evidence_collection_method,example_artifact,format,framework_category,mitigation_if_failed,points_assigned,responsible_role,row_id,score_if_passed,scoring_method,software_system_to_integrate,source_reference,target_ai_asset,updated_at) VALUES
	 ('74fa4f94-e5ef-49f9-b8b6-67ec27d8fac2'::uuid,'Operational','GOVERN 3.1','Diverse AI Risk Team','2025-12-03 19:12:27.062326','Decision-making for mapping, measuring, and managing AI risks is informed by a diverse team (demographics, disciplines, backgrounds).','Manual Upload','Policy Document','PDF','GOVERN (Governance)','Form cross-functional groups for AI risk review (including representatives from legal, ethics, technical, customer-facing roles, and with demographic diversity) to evaluate AI systems at design and post-deployment; track involvement to ensure broad input.',5,'AI Risk Officer',11,5,'Full points if multidisciplinary and diverse perspectives are systematically included (e.g. AI ethics committee with varied members) in AI risk processes (partial if teams are siloed or homogeneous).','ServiceNow','NIST AI RMF 1.0 – Govern 3.1','Organization (Workforce Diversity)','2025-12-03 19:12:27.062326'),
	 ('9440978e-77b3-434c-b5bd-9993ad0b0b40'::uuid,'Operational','GOVERN 3.2','Human-AI Oversight Roles','2025-12-03 19:12:27.062326','Policies and procedures define and differentiate roles and responsibilities for human-AI configurations and oversight of AI systems.','Automated Workflow','Training Records','XLSX','GOVERN (Governance)','Establish clear criteria for human oversight in AI operations (e.g. require human review of AI decisions in high-stakes cases) and assign specific staff or roles (like human reviewers or AI controllers) to perform these oversight functions; document these in SOPs.',5,'Compliance Manager',12,5,'Full points if there are guidelines on when human oversight or human-in-the-loop is required and who performs it (partial if reliance on ad-hoc human intervention).','Jira','NIST AI RMF 1.0 – Govern 3.2','Organization (Workforce & Process)','2025-12-03 19:12:27.062326'),
	 ('f12fa9ae-0e3c-45df-b0af-3a56a773e620'::uuid,'Operational','GOVERN 4.1','Risk-Aware Culture','2025-12-03 19:12:27.062326','Organizational policies and practices foster a critical thinking and safety-first mindset in AI design, development, deployment, and use to minimize negative impacts.','Third-Party Integration','Risk Register','CSV','GOVERN (Governance)','Promote an AI safety culture via top-down communication (e.g. leadership statements on responsible AI), include AI risk management in performance metrics and project gating (no launch if risks are high), and encourage staff to flag ethical or safety concerns without fear of reprisal (e.g. a “whistleblower” mechanism for AI issues).',5,'Data Protection Officer',13,5,'Full points if leadership messaging, incentives, and processes encourage raising AI risk concerns and prioritizing safety (partial if culture issues like pressure to deploy quickly at expense of risk exist).','Confluence','NIST AI RMF 1.0 – Govern 4.1','Organization (Culture)','2025-12-03 19:12:27.062326'),
	 ('016d8ab8-fff8-4243-95bc-836b374ee832'::uuid,'Operational','GOVERN 4.2','Document & Communicate AI Risks','2025-12-03 19:12:27.062326','Teams document the risks and potential impacts of the AI technology they design, develop, deploy, and use, and communicate about those impacts broadly.','Manual Entry','System Design Spec','DOCX','GOVERN (Governance)','Require AI project teams to create an “AI Risk and Impact Report” outlining identified risks, test results (e.g. bias, safety), and mitigation plans. Share summaries of these reports with relevant stakeholders (e.g. publish externally when appropriate or circulate internally to all impacted departments).',5,'Product Manager',14,5,'Full points if for each AI system a documented risk assessment exists and key findings (e.g. intended use, limitations) are shared with stakeholders (partial if documentation exists but not communicated, or sporadic).','OneTrust','NIST AI RMF 1.0 – Govern 4.2','Organization (Transparency)','2025-12-03 19:12:27.062326'),
	 ('b5b1ddc8-957c-4b22-849a-6f1aed395dfc'::uuid,'Technical/Operational','GOVERN 4.3','Enable Testing & Incident Sharing','2025-12-03 19:12:27.062326','Organizational practices enable AI testing, incident identification, and information sharing (e.g. about failures).','System Sync','Audit Trail','LOG','GOVERN (Governance)','Institute formal AI system testing protocols (including adversarial testing and bias audits) before and after deployment. Create an internal AI incident database to log any failures or near-misses; regularly review these incidents and update guidelines or models accordingly. Participate in industry info-sharing groups on AI incidents if possible to stay ahead of common failures.',5,'Audit Lead',15,5,'Full points if a robust system for testing AI (pre-deployment validation, red-teaming) and capturing AI incidents (with a lessons-learned process) is in place (partial if only minimal testing or no incident repository).','RiskLens','NIST AI RMF 1.0 – Govern 4.3','Organization (Learning & Improvement)','2025-12-03 19:12:27.062326'),
	 ('c03c445b-e22b-4430-8eca-9f2b0edc20c8'::uuid,'Operational','GOVERN 5.1','External Stakeholder Engagement','2025-12-03 19:12:27.062326','Policies and practices exist to collect, consider, prioritize, and integrate feedback from external stakeholders regarding potential individual and societal impacts of AI systems.','Manual Upload','Stakeholder Feedback Log','PDF','GOVERN (Governance)','Set up engagement channels such as user committees, public comment periods, or collaborations with external ethics boards to gather input on AI system impact. Establish a process to review this feedback and translate it into system improvements or mitigation measures.',5,'AI Risk Officer',16,5,'Full points if the organization actively solicits and incorporates external feedback (from users, civil society, affected communities) on AI impacts (partial if feedback is passive or not acted upon).','ServiceNow','NIST AI RMF 1.0 – Govern 5.1','External Stakeholders (Community)','2025-12-03 19:12:27.062326'),
	 ('d482ef51-00e0-4a51-9f43-9d38112c627b'::uuid,'Operational','GOVERN 5.2','Feedback Integration Mechanisms','2025-12-03 19:12:27.062326','Mechanisms are established for AI development/deployment teams to regularly incorporate adjudicated feedback from relevant AI actors into system design and implementation.','Automated Workflow','Incident Report','XLSX','GOVERN (Governance)','Implement a periodic review in each AI team’s workflow to consider feedback (user complaints, audit findings, etc.). For example, every quarter, product teams must review any stakeholder feedback and document decisions on whether to modify models or processes; tie this into the project management system so it cannot be overlooked.',5,'Compliance Manager',17,5,'Full points if there is a defined feedback loop (e.g. periodic reviews or updates) where stakeholder feedback or incident reports result in concrete changes to AI systems (partial if feedback is collected but seldom leads to changes).','Jira','NIST AI RMF 1.0 – Govern 5.2','Product Teams (AI Development)','2025-12-03 19:12:27.062326'),
	 ('6cfd5869-9a11-4e0c-94b4-edc82a6a867d'::uuid,'Operational','GOVERN 6.1','Third-Party AI Risk Policies','2025-12-03 19:12:27.062326','Policies and procedures address AI risks associated with third-party entities, including IP infringement or other external risks.','Third-Party Integration','Change Request Form','CSV','GOVERN (Governance)','Develop specific guidelines for using third-party AI resources: e.g. require legal/IP review for external data and models, security scans for third-party AI software, and impose contractual requirements on vendors to adhere to your AI risk standards. Maintain a list of approved third-party AI components that have passed these checks.',5,'Data Protection Officer',18,5,'Full points if third-party AI components (libraries, models, data) are governed by policy (covering licensing, security vetting, etc.) (partial if third-party risks are not formally managed).','Confluence','NIST AI RMF 1.0 – Govern 6.1','Third-Party Components','2025-12-03 19:12:27.062326'),
	 ('a84488a2-c6c2-4d9d-a248-c583da02e41c'::uuid,'Operational','GOVERN 6.2','Third-Party Incident Contingency','2025-12-03 19:12:27.062326','Contingency processes are in place to handle failures or incidents in third-party data or AI systems deemed high-risk.','Manual Entry','Executive Sign-off Memo','DOCX','GOVERN (Governance)','Extend your AI incident response plan to cover third-party issues: for example, if an external AI service misbehaves or is unavailable, have fallback procedures (like switch to backup service or disable AI feature gracefully). Test these contingency plans periodically and ensure responsible teams know how to execute them.',5,'Product Manager',19,5,'Full points if incident response plans include scenarios like “third-party model service goes down or produces bad output,” with defined actions (partial if such contingencies have not been considered).','OneTrust','NIST AI RMF 1.0 – Govern 6.2','Third-Party Components','2025-12-03 19:12:27.062326'),
	 ('8085afc7-2e7c-4421-831d-10abfabea868'::uuid,'Operational','MAP 1.1','Define AI Context & Purpose','2025-12-03 19:12:27.062326','Intended purposes, potentially beneficial uses, context-specific laws, norms, and deployment settings for the AI system are understood and documented.','System Sync','Data Lineage Report','LOG','MAP (Map Context)','Create an AI system context checklist to be completed at project initiation: document the system’s intended purpose, likely users, operating environment, and any legal or societal norms applicable. Use this document to guide design and risk assessments.',5,'Audit Lead',20,5,'Full points if for each AI project a context document exists (including use cases, benefits, constraints, applicable laws) (partial if context is poorly defined beyond technical spec).','RiskLens','NIST AI RMF 1.0 – Map 1.1','AI System (Use Case Definition)','2025-12-03 19:12:27.062326');
INSERT INTO public.compliance_master_nist (id,classification,control_id,control_name,created_at,description,evidence_collection_method,example_artifact,format,framework_category,mitigation_if_failed,points_assigned,responsible_role,row_id,score_if_passed,scoring_method,software_system_to_integrate,source_reference,target_ai_asset,updated_at) VALUES
	 ('fe4d439a-6aaf-402b-90f5-9706f0af46c6'::uuid,'Operational','MAP 1.2','Interdisciplinary Expertise in Context','2025-12-03 19:12:27.062326','Interdisciplinary AI actors with diverse competencies and experiences are involved in establishing the context, and their participation is documented (collaboration opportunities are prioritized).','Manual Upload','Policy Document','PDF','MAP (Map Context)','When scoping an AI system, convene a workshop with participants from various backgrounds (e.g. subject matter experts, potential users, risk officers) to discuss context and assumptions. Document the attendees and their contributions, and integrate their insights into requirements and design.',5,'AI Risk Officer',21,5,'Full points if context definition involves input from diverse fields (e.g. domain experts, end-users, ethicists) and is not done by developers alone (partial if little outside input).','ServiceNow','NIST AI RMF 1.0 – Map 1.2','AI System (Context Setting)','2025-12-03 19:12:27.062326'),
	 ('aca2b932-08f0-415c-94ed-8b47e5df0637'::uuid,'Operational','MAP 1.3','Align with Organization Mission','2025-12-03 19:12:27.062326','The organization’s mission and relevant goals for AI technology are understood and documented in the context of the system.','Automated Workflow','Training Records','XLSX','MAP (Map Context)','For each AI system, explicitly map its purpose to the organization’s strategic objectives or values (e.g. “This AI chatbot will improve customer service response time by 50%, aligning with our goal of customer satisfaction”). Include this mapping in project documentation to ensure the AI’s development remains mission-focused.',5,'Compliance Manager',22,5,'Full points if there is a clear statement of how the AI system supports business or mission objectives (partial if alignment is unclear or not articulated).','Jira','NIST AI RMF 1.0 – Map 1.3','Organization (Strategy Alignment)','2025-12-03 19:12:27.062326'),
	 ('c56a7b09-bff5-4811-9229-d31926278036'::uuid,'Operational','MAP 1.4','Business Value Context','2025-12-03 19:12:27.062326','The business value or context of use has been clearly defined or (for existing systems) re-evaluated.','Third-Party Integration','Risk Register','CSV','MAP (Map Context)','Develop a business case for the AI system detailing expected benefits (ROI, efficiency gains, etc.) and any contextual business constraints. For systems already in production, schedule periodic re-evaluation (e.g. annually) to ensure the AI is still delivering value and aligned with current business context.',5,'Data Protection Officer',23,5,'Full points if a business case or value proposition document exists and is periodically revisited for each AI system (partial if only initially considered or not at all).','Confluence','NIST AI RMF 1.0 – Map 1.4','Organization (Strategy Alignment)','2025-12-03 19:12:27.062326'),
	 ('b265533e-40f4-4e39-80ed-9ca95b737935'::uuid,'Operational','MAP 1.5','Risk Tolerances Determined','2025-12-03 19:12:27.062326','Organizational risk tolerances for the AI system (i.e. acceptable levels of risk) are determined and documented.','Manual Entry','System Design Spec','DOCX','MAP (Map Context)','During risk planning, define quantitative or qualitative thresholds for acceptable risk (e.g. “model accuracy must be at least X% for critical decisions” or “no more than Y% disparity between groups in outcomes”). Document these risk tolerance criteria and use them to guide mitigation decisions or system go/no-go decisions.',5,'Product Manager',24,5,'Full points if specific risk thresholds (e.g. maximum acceptable error rates, fairness metrics) are set for the AI system in line with corporate risk appetite (partial if only general statements like “low risk” without specifics).','OneTrust','NIST AI RMF 1.0 – Map 1.5','Organization (Risk Appetite)','2025-12-03 19:12:27.062326'),
	 ('bfdcb3f1-abcc-4c22-89c7-e1b435f0bfc6'::uuid,'Technical/Operational','MAP 1.6','Requirements Elicitation (Socio-technical)','2025-12-03 19:12:27.062326','System requirements (including socio-technical considerations) are elicited from relevant AI actors, and design decisions account for these to address AI risks.','System Sync','Audit Trail','LOG','MAP (Map Context)','Expand the requirements gathering phase to include questions about ethical and social implications (e.g. privacy requirements, accessibility needs, fail-safe behaviors). Interview or survey various stakeholders (users, risk officers, etc.) for their requirements. Ensure that the final requirements documentation includes sections for these socio-technical criteria and that they influence design choices (e.g. opting for a simpler model for transparency).',5,'Audit Lead',25,5,'Full points if requirements gathering includes not just technical specs but also ethical, user experience, and risk requirements from stakeholders (partial if requirements are purely technical).','RiskLens','NIST AI RMF 1.0 – Map 1.6','AI System (Requirements)','2025-12-03 19:12:27.062326'),
	 ('753afc8e-dd1f-49ab-ab94-7b74fc5d382f'::uuid,'Technical/Operational','MAP 2.2','Knowledge Limits Documentation','2025-12-03 19:12:27.062326','Information about the AI system’s knowledge limits and how its outputs will be used and overseen by humans is documented.','Manual Upload','Stakeholder Feedback Log','PDF','MAP (Map Categorization)','Document the boundaries of the AI system’s competence (e.g. “model is only reliable for English text, fails on slang”; “not trained for users under 13”). Also specify how human operators will use the output – for instance, “results will be reviewed by a clinician before any patient action.” Ensure these constraints are communicated to users and integrated into SOPs.',5,'AI Risk Officer',27,5,'Full points if limitations of the model (when it might fail, types of inputs it can’t handle, etc.) are documented and plans for human oversight of outputs are in place (partial if limitations are not studied or no plan for misuse).','ServiceNow','NIST AI RMF 1.0 – Map 2.2','AI System (Capabilities & Limits)','2025-12-03 19:12:27.062326'),
	 ('0440a307-deb3-463a-8565-2f95dd37ff9d'::uuid,'Operational','MAP 3.1','Benefits Examined','2025-12-03 19:12:27.062326','Potential benefits of the AI system’s intended functionality and performance are examined and documented.','Automated Workflow','Incident Report','XLSX','MAP (Map Prioritization)','As part of risk assessment, also perform a benefit assessment: list the positive impacts the AI system aims to achieve (with metrics if possible). For example, “reduce false negatives by 20% compared to manual review” or “provide 24/7 support to 100k users.” Track these benefits during pilots or deployment to ensure the system is delivering value.',5,'Compliance Manager',29,5,'Full points if expected positive outcomes (e.g. efficiency gains, improved accuracy over current process) are explicitly identified and measured (partial if benefits are assumed but not quantified or tracked).','Jira','NIST AI RMF 1.0 – Map 3.1','AI System (Impact)','2025-12-03 19:12:27.062326'),
	 ('076b67f6-64a4-4efe-9eb2-804ce723d3f0'::uuid,'Operational','MAP 3.2','Costs & Harms Examined','2025-12-03 19:12:27.062326','Potential costs (including non-monetary, like negative impacts from errors or trust issues) associated with the AI system are examined and documented, relative to the organization’s risk tolerance.','Third-Party Integration','Change Request Form','CSV','MAP (Map Prioritization)','Perform a “cost of AI failure” analysis: e.g. estimate what an error would cost (in money, safety, or reputation). Consider worst-case scenarios. Document these potential costs and verify that leadership agrees these are tolerable given the expected benefits. If not, adjust system scope or add safeguards until residual risk is within tolerance.',5,'Data Protection Officer',30,5,'Full points if the analysis includes potential negatives (financial cost of errors, reputational damage, user harm) and if those are weighed against risk tolerance (partial if only financial cost considered or none at all).','Confluence','NIST AI RMF 1.0 – Map 3.2','AI System (Impact)','2025-12-03 19:12:27.062326'),
	 ('f790ac2a-61b1-4b63-9d38-fb440204f0b7'::uuid,'Operational','MAP 3.3','Application Scope Defined','2025-12-03 19:12:27.062326','The targeted application scope is specified and documented based on the system’s capability, context, and categorization.','Manual Entry','Executive Sign-off Memo','DOCX','MAP (Map Prioritization)','Write a scope statement for the AI system that includes domains of use and any forbidden contexts. For instance, “This face recognition model is approved for unlocking personal devices, not for law enforcement identification.” Ensure any use outside this scope triggers a re-evaluation. Communicate the scope to all stakeholders and monitor for off-label uses.',5,'Product Manager',31,5,'Full points if a clear statement of where and how the AI will be applied (and equally important, not applied) is written (partial if scope creep is likely because scope isn’t well-defined).','OneTrust','NIST AI RMF 1.0 – Map 3.3','AI System (Use Scope)','2025-12-03 19:12:27.062326'),
	 ('daa61035-93f2-43fd-bf15-6f2a42a1a04b'::uuid,'Operational','MAP 3.4','Operator Proficiency Processes','2025-12-03 19:12:27.062326','Processes for ensuring operator/practitioner proficiency with the AI system’s performance and trustworthiness (including relevant technical standards or certifications) are defined, assessed, and documented.','System Sync','Data Lineage Report','LOG','MAP (Map Prioritization)','Develop a training curriculum for users/operators of the AI system focusing on understanding model outputs, recognizing when the AI might be wrong, and knowing how to intervene. If applicable, require an internal certification or quiz completion before someone can deploy or use the AI in critical processes. Keep records of trained personnel and periodically refresh their training, especially after system updates.',5,'Audit Lead',32,5,'Full points if there is a training or certification program for those who will operate or manage the AI, and a way to assess their understanding of the system’s limitations and proper use (partial if operators receive no special training).','RiskLens','NIST AI RMF 1.0 – Map 3.4','Organization (Training/Skills)','2025-12-03 19:12:27.062326');
INSERT INTO public.compliance_master_nist (id,classification,control_id,control_name,created_at,description,evidence_collection_method,example_artifact,format,framework_category,mitigation_if_failed,points_assigned,responsible_role,row_id,score_if_passed,scoring_method,software_system_to_integrate,source_reference,target_ai_asset,updated_at) VALUES
	 ('b3a2aa74-1cf7-422b-92d5-3c196e1e0712'::uuid,'Operational','MAP 3.5','Human Oversight Processes','2025-12-03 19:12:27.062326','Processes for human oversight of the AI system are defined, assessed, and documented, in accordance with organizational policies.','Manual Upload','Policy Document','PDF','MAP (Map Prioritization)','Establish formal human-in-the-loop or human-on-the-loop procedures: e.g. real-time monitoring dashboards for AI decisions, a threshold for human review (like any credit decision for loans above $100k must be approved by a person), and an override mechanism. Document these in an oversight plan and verify they align with any governance policies (like an AI ethics policy requiring human final say in certain cases). Periodically test the oversight process (e.g. drills or audits of decisions).',5,'AI Risk Officer',33,5,'Full points if clear procedures exist for how humans monitor and can intervene in the AI’s operation (partial if oversight responsibilities are undefined or purely reactive after issues occur).','ServiceNow','NIST AI RMF 1.0 – Map 3.5','AI System (Oversight)','2025-12-03 19:12:27.062326'),
	 ('90d80760-6bde-41a9-a462-de1013030e48'::uuid,'Operational','MAP 4.1','Map AI & Legal Risks (incl. Supply Chain)','2025-12-03 19:12:27.062326','Approaches for mapping AI technology and legal risks of system components – including third-party data/software – are in place, followed, and documented (including IP infringement risks).','Automated Workflow','Training Records','XLSX','MAP (Map System Risk)','Perform a component-by-component risk analysis: data (privacy, bias risks?), model (security vulnerabilities?), platform (reliability?), third-party tools (license or IP risks?). Document these risk maps. For legal risks, coordinate with legal counsel to check IP usage rights (for datasets, pretrained models) and compliance requirements for each component. Ensure the risk map is reviewed before deployment and kept updated with any integration of new components.',5,'Compliance Manager',34,5,'Full points if there''s a systematic risk mapping for each component of the AI (data, model, hardware, vendor services) and legal review (partial if only core model considered, ignoring data or external elements).','Jira','NIST AI RMF 1.0 – Map 4.1','AI System (Supply Chain & IP)','2025-12-03 19:12:27.062326'),
	 ('391eabec-3345-4712-8025-5c8a19050def'::uuid,'Operational','MAP 5.1','Impact Likelihood & Magnitude Characterized','2025-12-03 19:12:27.062326','The likelihood and magnitude of each identified potential impact (beneficial or harmful), based on expected use, past incidents, external feedback, etc., are characterized.','Third-Party Integration','Risk Register','CSV','MAP (Map Impact)','Use a risk assessment matrix for AI impacts: for each negative impact identified (and even positive opportunities), assign a likelihood (based on historical data, testing, or expert judgment) and an impact severity. Use sources like past incident databases or user feedback to inform this. Document these ratings and ensure the reasoning (why a risk is considered High vs Medium, etc.) is noted. Update these characterizations as new data comes in (e.g. incidents or near-misses).',5,'Data Protection Officer',36,5,'Full points if for each identified risk or impact, there is an assessment of how likely it is and how severe it could be, using data or expert input (partial if impacts listed but not rated, or guesswork with no rationale).','Confluence','NIST AI RMF 1.0 – Map 5.1','AI System (Impact Analysis)','2025-12-03 19:12:27.062326'),
	 ('7291eec2-220d-45d4-a1fb-035e33fc44ca'::uuid,'Operational','MAP 5.2','Ongoing Stakeholder Feedback Loops','2025-12-03 19:12:27.062326','Practices and personnel for regular engagement with relevant AI actors and integrating feedback about positive, negative, and unanticipated impacts are in place and documented.','Manual Entry','System Design Spec','DOCX','MAP (Map Impact)','Assign a team or role (e.g. “AI Product Steward”) responsible for collecting ongoing feedback from various stakeholders (customers, impacted community members, domain experts) on the AI system’s effects. Set up recurring activities (like quarterly stakeholder meetings or surveys). Ensure the feedback is formally reviewed and feeds into system updates or risk re-evaluation. Document these engagements and any changes made as a result.',5,'Product Manager',37,5,'Full points if a continuous feedback loop is established (e.g. periodic user forums, community surveys, or expert audits, with assigned staff to handle feedback) (partial if feedback is only reactive after something goes wrong).','OneTrust','NIST AI RMF 1.0 – Map 5.2','Organization (Stakeholder Input)','2025-12-03 19:12:27.062326'),
	 ('68efb9fc-a30d-469a-bd57-f1336af75f5e'::uuid,'Operational','MEASURE 1.2','Assess Metric Efficacy','2025-12-03 19:12:27.062326','The appropriateness of AI metrics and the effectiveness of existing controls are regularly assessed and updated, including considering error reports and impacts on communities.','System Sync','Audit Trail','LOG','MEASURE (Measure Risk)','Establish a schedule (e.g. after each deployment or quarterly) to review AI performance metrics and control outcomes. For example, if your metric was accuracy, but users still report issues, maybe you need a new metric like calibration or user satisfaction. Adjust metrics as needed and strengthen controls if data (like error logs or user complaints) shows they aren’t effective. Document these reviews and changes in the risk management log.',5,'Audit Lead',39,5,'Full points if there is a periodic review (e.g. model performance committee) that evaluates whether current metrics truly capture what is important and whether controls are working (partial if metrics once set are never revisited, or no process to evaluate control effectiveness).','RiskLens','NIST AI RMF 1.0 – Measure 1.2','Organization (Metrics Governance)','2025-12-03 19:12:27.062326'),
	 ('5c40f40d-fa41-4d4e-98f7-4b0c797fce87'::uuid,'Operational','MEASURE 1.3','Independent Assessment Involvement','2025-12-03 19:12:27.062326','Internal experts not on the development team, and/or independent assessors, regularly participate in AI risk assessments and updates. Domain experts, users, external AI actors, and affected communities are consulted as needed in these assessments.','Manual Upload','Stakeholder Feedback Log','PDF','MEASURE (Measure Risk)','Implement an AI review process that involves a neutral party: for instance, create an AI oversight committee or use an internal audit function to evaluate the AI system’s risk controls and metrics. Also, bring in domain experts or end-users to validate if the system is meeting requirements safely. Ensure these reviewers have access to documentation and can challenge the team’s assumptions. Document their feedback and track the closure of any issues they identify.',5,'AI Risk Officer',40,5,'Full points if each AI system undergoes review by parties other than its creators (e.g. internal audit, external auditor, or a separate QA team) and if outside perspectives (users, community) are sought appropriately (partial if all assessment is done only by the dev team).','ServiceNow','NIST AI RMF 1.0 – Measure 1.3','Organization (Independent Oversight)','2025-12-03 19:12:27.062326'),
	 ('769dbf1c-c335-4f1b-af67-8708c9e64f9b'::uuid,'Operational','MEASURE 2.2','Human Subject Evaluations','2025-12-03 19:12:27.062326','Evaluations involving human subjects meet applicable requirements (e.g. IRB ethics approvals) and are representative of the relevant population.','Automated Workflow','Incident Report','XLSX','MEASURE (Evaluate Trustworthiness)','If your AI system’s performance or safety is being evaluated through user studies or pilots (especially with vulnerable populations), ensure compliance with human subject research standards (like obtaining consent, IRB review if academic/medical). Also, recruit participants that reflect the diversity of real end-users (in terms of demographics, skill levels, etc.) so results generalize. Document the evaluation protocol and approvals.',5,'Compliance Manager',42,5,'Full points if any user testing or human trial of the AI (for example in healthcare or HR contexts) follows ethical guidelines and has a sample reflective of the actual user base (partial if human testing is done but without ethics review or using unrepresentative testers).','Jira','NIST AI RMF 1.0 – Measure 2.2','AI System (Validation)','2025-12-03 19:12:27.062326'),
	 ('1712f491-c432-4e39-86be-694b3d37dbee'::uuid,'Operational','MEASURE 2.8','Transparency & Accountability Evaluated','2025-12-03 19:12:27.062326','Risks related to transparency and accountability (as identified in MAP) are examined and documented.','Third-Party Integration','Change Request Form','CSV','MEASURE (Evaluate Trustworthiness)','Conduct a transparency audit: verify if the AI system can provide explanations or audit logs for its decisions. For example, ensure important model decisions are accompanied by rationale or that the system logs all inputs/outputs for later review. Check accountability structures: e.g. is there an owner for the AI’s output (a human who signs off)? Evaluate if these measures meet any regulatory or ethical requirements for transparency. Document any gaps (like “cannot fully explain decisions to lay users”) and plan mitigations (like a simplified explanation interface or documentation improvements).',5,'Data Protection Officer',48,5,'Full points if an audit or review checks whether the system’s operations can be understood and traced (e.g. decisions are logged, explanations can be generated) and if mechanisms exist to hold the system or people accountable for its decisions (partial if these aspects not evaluated).','Confluence','NIST AI RMF 1.0 – Measure 2.8','AI System (Accountability)','2025-12-03 19:12:27.062326'),
	 ('9a88c128-c136-4d79-bed3-2826cc4f09a3'::uuid,'Technical/Operational','MEASURE 2.10','Privacy Risk Assessed','2025-12-03 19:12:27.062326','Privacy risks of the AI system (as identified in MAP) are examined and documented.','Manual Entry','Executive Sign-off Memo','DOCX','MEASURE (Evaluate Trustworthiness)','Perform a Privacy Impact Assessment on the AI system: identify all personal data being used, how it flows through the system, and any privacy concerns (e.g. sensitive attributes inferred, data retention, unintended memorization by the model). Document compliance with applicable privacy laws (GDPR, etc.). Implement mitigations such as data anonymization, differential privacy techniques, or strict access controls as needed. Summarize the privacy findings and actions in a PIA report and revisit it whenever the AI or its data sources change.',5,'Product Manager',50,5,'Full points if a privacy impact assessment (PIA) or equivalent has been done for the AI, addressing issues like personal data usage, potential for data re-identification, etc., and mitigations implemented (partial if personal data is involved but no thorough privacy analysis done).','OneTrust','NIST AI RMF 1.0 – Measure 2.10','AI System (Privacy)','2025-12-03 19:12:27.062326'),
	 ('de1765fd-1593-4cd1-99eb-4ac56bd2b350'::uuid,'Operational','MEASURE 2.12','Environmental Impact Assessed','2025-12-03 19:12:27.062326','Environmental impact and sustainability of AI model training and management – as identified in MAP – are assessed and documented.','System Sync','Data Lineage Report','LOG','MEASURE (Evaluate Trustworthiness)','Calculate or estimate the compute resources and energy consumption of the AI system, particularly for model training and large-scale deployments. Document these findings (e.g. “Training this model emitted ~50 kg CO2”). If high, explore optimizations: model distillation, efficient architectures, using renewable-energy cloud regions, etc. Set internal guidelines for acceptable resource usage (especially if multiple approaches can solve the problem). Track this metric over time, and include it in decision-making (for example, if two models have similar performance, prefer the one with less environmental impact).',5,'Audit Lead',52,5,'Full points if the team has considered and estimated the carbon footprint or energy usage of the AI (especially during training or heavy computation) and has looked for ways to reduce it (partial if energy use is huge but no awareness or mitigation considered).','RiskLens','NIST AI RMF 1.0 – Measure 2.12','AI System (Sustainability)','2025-12-03 19:12:27.062326');
INSERT INTO public.compliance_master_nist (id,classification,control_id,control_name,created_at,description,evidence_collection_method,example_artifact,format,framework_category,mitigation_if_failed,points_assigned,responsible_role,row_id,score_if_passed,scoring_method,software_system_to_integrate,source_reference,target_ai_asset,updated_at) VALUES
	 ('8d457389-4b52-40a4-9a79-d6ba0e2e6c25'::uuid,'Operational','MEASURE 2.13','TEVV Process Effectiveness Evaluated','2025-12-03 19:12:27.062326','The effectiveness of the measurement (TEVV) metrics and processes themselves is evaluated and documented.','Manual Upload','Policy Document','PDF','MEASURE (Evaluate Trustworthiness)','After deploying the AI system (or after several iterations), conduct a retrospective on the evaluation process: did our test metrics truly predict real-world success? For example, maybe the model had high accuracy but users are unhappy – indicating our metrics missed something (like user trust). Document these insights and update the evaluation framework (e.g. add new metrics such as user satisfaction, or adjust test datasets). This continuous improvement of the testing methodology should be part of quality management reviews.',5,'AI Risk Officer',53,5,'Full points if the organization periodically reflects on whether their evaluation methods are adequate or need improvement (partial if once metrics set, they’re never revisited even if tech evolves).','ServiceNow','NIST AI RMF 1.0 – Measure 2.13','Organization (Process Improvement)','2025-12-03 19:12:27.062326'),
	 ('caab7c40-eff2-4b9d-8569-19d7e41677e2'::uuid,'Operational','MEASURE 3.1','Risk Tracking Established','2025-12-03 19:12:27.062326','Approaches, personnel, and documentation are in place to regularly identify and track existing, new, and emergent AI risks based on performance in deployed contexts.','Automated Workflow','Training Records','XLSX','MEASURE (Monitor Risks)','Assign responsibility (e.g. the AI product manager or a risk analyst) to monitor deployed AI systems for new risks or degradation. This can include reviewing logs, user feedback, model performance metrics, and external news (e.g. new vulnerabilities in similar AI). Maintain a living risk register that gets updated with any new risks or changes in existing ones. Meet regularly (like an AI risk review meeting) to discuss if new controls are needed for observed issues. Document these meetings and decisions.',5,'Compliance Manager',54,5,'Full points if a formal post-deployment risk monitoring process exists (e.g. someone reviews production incidents or drift data monthly and updates risk register) (partial if after deployment, risks are basically not revisited until a major problem occurs).','Jira','NIST AI RMF 1.0 – Measure 3.1','Organization (Risk Monitoring)','2025-12-03 19:12:27.062326'),
	 ('4eca5c12-96b2-48ad-a7a4-6452a0e9c160'::uuid,'Operational','MEASURE 3.2','Hard-to-Assess Risks Considered','2025-12-03 19:12:27.062326','Risk tracking approaches are considered for scenarios where AI risks are hard to measure with current techniques or metrics are not available.','Third-Party Integration','Risk Register','CSV','MEASURE (Monitor Risks)','For each AI system, list any known “unknowns” – aspects you suspect could be risky but lack metrics or methods to evaluate (e.g. subtle societal impacts, long-term effects). Stay engaged with research and industry groups on these topics. Perhaps implement qualitative monitoring – e.g. periodically get expert opinions or user narratives to sense any issues. Update your risk management when new techniques or data become available (for example, if a new bias detection method is invented, adopt it to measure a previously unmeasurable bias). This forward-looking stance should be documented as part of your risk strategy.',5,'Data Protection Officer',55,5,'Full points if the organization explicitly acknowledges areas of uncertainty (like “we don’t yet know how to measure consciousness or long-term social effect”) and sets a plan to keep an eye on those or update when possible (partial if such areas exist but are ignored).','Confluence','NIST AI RMF 1.0 – Measure 3.2','Organization (Risk Monitoring)','2025-12-03 19:12:27.062326'),
	 ('b4041ae1-b931-47d3-81d0-bba16a310fd9'::uuid,'Operational','MEASURE 3.3','Feedback Channels for Problems','2025-12-03 19:12:27.062326','Feedback processes for end users and affected communities to report problems and appeal outcomes are established and integrated into AI system evaluation metrics.','Manual Entry','System Design Spec','DOCX','MEASURE (Monitor Risks)','Provide end-users with accessible means to report AI errors or unfair outcomes (like a “Was this answer helpful?” prompt, or a dedicated support line for AI decisions). Also implement an appeal process if the AI affects rights (for instance, “if you think this automated decision was wrong, a human will review it”). Feed these reports into your evaluation: track number and type of complaints as a metric for model performance. Regularly analyze this feedback to identify areas for model or process improvement.',5,'Product Manager',56,5,'Full points if users have a clear way to flag errors or contest AI decisions, and such feedback is logged and considered in performance reviews (partial if no user recourse mechanism exists or is not used in evaluating system).','OneTrust','NIST AI RMF 1.0 – Measure 3.3','External Stakeholders (Users)','2025-12-03 19:12:27.062326'),
	 ('0f91e4ae-cc4e-4af3-98ff-00e9365d4997'::uuid,'Operational','MEASURE 4.1','Contextualized Measurement Approaches','2025-12-03 19:12:27.062326','Measurement approaches for identifying AI risks are linked to the deployment context(s) and informed by consultation with domain experts and end users, and are documented.','System Sync','Audit Trail','LOG','MEASURE (Improve Measures)','Customize your evaluation metrics to each deployment scenario. For example, an AI vision system in a factory might need a much higher precision (because a false negative could be dangerous) compared to the same model used for a casual smartphone app. Talk to field experts or end-users about what success and failure mean in their environment and adjust metrics/thresholds accordingly. Document these context-specific metrics in the evaluation plan. Ensure this consultation is done upfront and whenever the context changes (new use-case or environment for the AI).',5,'Audit Lead',57,5,'Full points if metrics and evaluation strategies are not one-size-fits-all but tailored to how and where the AI is used (with input from those intimately familiar with that context) (partial if generic metrics are used without regard to context differences).','RiskLens','NIST AI RMF 1.0 – Measure 4.1','Organization (Metrics Governance)','2025-12-03 19:12:27.062326'),
	 ('efe91ddf-ffbe-4c23-ae35-91eb01358e05'::uuid,'Operational','MEASURE 4.2','Stakeholder-Validated Measurement Results','2025-12-03 19:12:27.062326','Measurement results regarding AI system trustworthiness in deployment are validated with input from domain experts and relevant AI actors, to check the system is performing as intended; results are documented.','Manual Upload','Stakeholder Feedback Log','PDF','MEASURE (Improve Measures)','After deployment, don’t just trust the numbers; engage with external perspectives. For instance, if your metrics say the AI voice assistant has a 2% error rate, have a focus group of users or linguists evaluate some interactions to confirm that the errors truly are low and acceptable. Or if you claim fairness, bring in an auditor or affected community representatives to review results. Document this external validation – maybe as an audit report or appendix to your performance evaluation. If discrepancies are found (experts say quality is worse than metrics suggest), adjust either the system or your measurement approach.',5,'AI Risk Officer',58,5,'Full points if after deploying, the organization seeks external validation of performance (e.g. an expert audit or user survey to see if the metrics really reflect user experience) and if those insights are recorded (partial if internal metrics look good but the system is actually causing issues that experts/users would catch, but they’re not consulted).','ServiceNow','NIST AI RMF 1.0 – Measure 4.2','Organization (Metrics Governance)','2025-12-03 19:12:27.062326'),
	 ('e171abf4-6ec0-427f-9d45-6f6ef95936ed'::uuid,'Operational','MEASURE 4.3','Continuous Improvement via Feedback','2025-12-03 19:12:27.062326','Measurable performance improvements or declines, based on input from relevant AI actors (including affected communities) and field data on context-relevant risks and characteristics, are identified and documented.','Automated Workflow','Incident Report','XLSX','MEASURE (Improve Measures)','Establish a formal continuous improvement cycle: for example, after each quarter or major incident, gather all relevant feedback (user complaints, incident reports, drift data) and analyze trends. If a decline in performance is noted or a new risk emerged (maybe model accuracy dropped due to concept drift), plan an improvement (retrain model, tweak feature, etc.). Conversely, track improvements (did retraining on new data reduce error rates for the flagged cases?). Keep a log of changes made to the AI system and reasons (like an engineering journal). This ensures accountability and learning – showing over time how the system becomes more robust or where challenges persist.',5,'Compliance Manager',59,5,'Full points if the organization actively uses feedback and operational data to drive updates (e.g. model retraining, new features) and tracks these changes over time (partial if once system is out, improvements are ad-hoc or not tied to feedback systematically).','Jira','NIST AI RMF 1.0 – Measure 4.3','Organization (Continuous Improvement)','2025-12-03 19:12:27.062326'),
	 ('f34f51a8-7550-435b-af6c-d480f92b4c87'::uuid,'Technical','MAP 2.1','Task & Method Identification','2025-12-03 19:12:27.062326','The specific tasks the AI system will perform and the methods (e.g. classifiers, generative models) to implement them are defined and documented.',NULL,'AI System Task and Method Specification pack including problem statement, in scope and out of scope tasks, model type choice, rationale, and assumptions, plus approval record',NULL,'MAP (Map Categorization)','Clearly document the AI system’s functional scope (e.g. “identify objects in images” or “predict equipment failure”) and the modeling approach chosen (e.g. “CNN classifier” or “random forest regression”). Include why that method is appropriate. This should be in the design or project charter and reviewed by stakeholders.',5,NULL,26,5,'Full points if a clear functional specification (what the AI does) and chosen AI method/algorithm is documented with rationale (partial if these are unclear or not documented).',NULL,'NIST AI RMF 1.0 – Map 2.1','AI System (Functionality)','2025-12-03 19:12:27.062326'),
	 ('d5e64b0e-3942-4e65-b150-201e629a30c0'::uuid,'Technical','MAP 2.3','Scientific Integrity & TEVV','2025-12-03 19:12:27.062326','Scientific integrity and Test, Evaluation, Verification, & Validation (TEVV) considerations are identified and documented, addressing vulnerabilities (data quality, construct validity, etc.).',NULL,'Scientific Integrity and TEVV Plan for the AI System, evaluation design and success criteria, train validation test strategy with leakage checks and representativeness notes, TEVV matrix mapping requirements and key risks to tests metrics and thresholds, known vulnerabilities and planned mitigations such as data quality bias drift and spurious signals, reproducibility details including tools and versions seeds run IDs and configs, limitations and post deployment monitoring plan, review and sign offs by AI owner ML lead and an independent reviewer, format as PDF or DOCX plus an XLSX matrix with links to run logs and dataset metadata',NULL,'MAP (Map Categorization)','Plan the AI system’s validation strategy during design: ensure datasets for testing reflect real-world conditions and that metrics truly measure the intended outcomes (construct validity). Document these evaluation methodologies, any assumptions, and how they mitigate common pitfalls (like overfitting or bias). If certain reliability aspects are hard to measure, acknowledge that and monitor post-deployment.',5,NULL,28,5,'Full points if experimental design is sound (e.g. using proper validation sets, avoiding data leakage) and documented, and any limitations of the evaluation approach are noted (partial if evaluation methods are not robust or not documented).',NULL,'NIST AI RMF 1.0 – Map 2.3','AI System (Validation)','2025-12-03 19:12:27.062326'),
	 ('0da479ab-ea82-4ddc-9081-4e306881a350'::uuid,'Technical','MAP 4.2','Internal Risk Controls Identified','2025-12-03 19:12:27.062326','Internal risk controls for components of the AI system, including third-party AI technologies, are identified and documented.',NULL,'Component Risk to Control Mapping matrix that lists each AI component including third party model, vector DB, tools, data pipeline, and UI, the risks per component, the selected internal controls, implementation status, and verification method',NULL,'MAP (Map System Risk)','For each risk uncovered in the system component risk mapping, select or design a control to mitigate it and record this mapping. Example: if risk = “training data tampering,” control = “digital signatures on training data”; if risk = “cloud service outage,” control = “on-premise failover instance.” Document these controls in the system design and ensure they are implemented or in plan. Regularly verify that these controls remain effective (e.g. through testing).',5,NULL,35,5,'Full points if specific controls/mitigations exist for each identified risk in 4.1 (e.g. encryption for data, adversarial training for model, redundancy for platform) (partial if controls not mapped or gaps exist).',NULL,'NIST AI RMF 1.0 – Map 4.2','AI System (Supply Chain & Tech)','2025-12-03 19:12:27.062326');
INSERT INTO public.compliance_master_nist (id,classification,control_id,control_name,created_at,description,evidence_collection_method,example_artifact,format,framework_category,mitigation_if_failed,points_assigned,responsible_role,row_id,score_if_passed,scoring_method,software_system_to_integrate,source_reference,target_ai_asset,updated_at) VALUES
	 ('6ede7226-1a44-4d85-957f-2056dc2d295e'::uuid,'Technical','MEASURE 1.1','Select Risk Metrics','2025-12-03 19:12:27.062326','Approaches and metrics for measuring the AI risks identified in MAP are selected for implementation, focusing first on the most significant risks.',NULL,'AI Risk Metrics Register mapping each high priority risk to one or more measurable metrics, the data source for measurement, thresholds, sampling frequency, alerting rules, owner, and escalation path',NULL,'MEASURE (Measure Risk)','Identify quantitative or qualitative metrics corresponding to each key risk (e.g., bias -> disparate impact ratio, security -> number of adversarial examples that succeed). Prioritize developing measurement methods for the top risks. Implement these measurements in the model evaluation phase or monitoring phase. If a risk cannot currently be measured, document that gap and perhaps plan R&D to address it.',5,NULL,38,5,'Full points if for each high-priority risk there is a defined metric or test (e.g. fairness metric, accuracy on edge cases) to measure it (partial if metrics are missing for some major risks or not defined at all).',NULL,'NIST AI RMF 1.0 – Measure 1.1','AI System (Risk Metrics)','2025-12-03 19:12:27.062326'),
	 ('2dff54d9-2aed-4b4f-86c7-208a3b357c8c'::uuid,'Technical','MEASURE 2.1','Document TEVV & Tools','2025-12-03 19:12:27.062326','Test sets, evaluation metrics, and details about tools used during Testing/Evaluation/Verification/Validation (TEVV) are documented.',NULL,'TEVV Documentation Bundle for a model release including test datasets used, dataset versions and splits, evaluation metrics, toolchain and versions, configuration parameters, test run IDs, and reproducibility notes',NULL,'MEASURE (Evaluate Trustworthiness)','Maintain an “AI Test Report” for each model version that includes: description of test data (with representativeness notes), the metrics computed (accuracy, ROC AUC, etc.), and the toolchain (e.g. “Scikit-learn vX.Y, custom bias audit script vZ”). This ensures reproducibility and transparency of evaluation. Store these reports in a repository for audit purposes.',5,NULL,41,5,'Full points if there is a testing documentation that lists what datasets were used for evaluation, what metrics were measured, and what software/tools (including their versions or settings) were used (partial if testing was done but not well-documented).',NULL,'NIST AI RMF 1.0 – Measure 2.1','AI System (Validation)','2025-12-03 19:12:27.062326'),
	 ('8a1d2596-a96f-43b9-b91e-783eef15558f'::uuid,'Technical','MEASURE 2.3','Performance Criteria Demonstrated','2025-12-03 19:12:27.062326','AI system performance or assurance criteria are measured (qualitatively/quantitatively) and demonstrated under conditions similar to the deployment context. Measures are documented.',NULL,'Deployment Context Validation Report showing staging or pilot results under production like conditions including load, latency, real traffic patterns, edge cases, pass fail criteria, and sign off',NULL,'MEASURE (Evaluate Trustworthiness)','Before full deployment, test the AI system in a staging environment or pilot program that mirrors real-world conditions (load, noise, unexpected inputs, etc.). For example, if deploying a robot, test it in a real facility, not just simulation. Verify it meets all performance and safety criteria in this context. Document the results, and if any context-specific issues emerged (like performance drop in noisy environment), adjust the system or criteria accordingly.',5,NULL,43,5,'Full points if the system has been tested in an environment that closely mimics production (or via field trial) and met the predefined criteria (partial if testing only in lab conditions that differ greatly or criteria not fully met/checked).',NULL,'NIST AI RMF 1.0 – Measure 2.3','AI System (Validation)','2025-12-03 19:12:27.062326'),
	 ('4337dd49-2d98-44c2-8991-6b9b81ca09db'::uuid,'Technical','MEASURE 2.4','Monitor in Production','2025-12-03 19:12:27.062326','The functionality and behavior of the AI system and its components – as identified in MAP – are monitored when in production.',NULL,'Production Monitoring Plan and Evidence including dashboards for drift, quality, safety signals, alert definitions, on call runbook, and incident tickets from triggered alerts',NULL,'MEASURE (Evaluate Trustworthiness)','Implement runtime monitoring for the AI system: for instance, track distribution of inputs and outputs to detect drift from training data, use anomaly detection on model confidence or error rates to catch unusual behavior, and monitor each component’s health (data pipeline errors, inference latency, etc.). Set thresholds that trigger alerts to engineers or suspend the AI’s operation if necessary. Regularly review monitoring logs in post-mortems or operational meetings.',5,NULL,44,5,'Full points if there’s an active monitoring solution for the AI’s outputs and performance in real time (drift detection, anomaly alerts, etc.), covering all critical components (partial if no or minimal runtime monitoring beyond basic uptime).',NULL,'NIST AI RMF 1.0 – Measure 2.4','AI System (Monitoring)','2025-12-03 19:12:27.062326'),
	 ('0c9bec58-358d-4733-9a48-5ba5df9126c9'::uuid,'Technical','MEASURE 2.5','Validity & Reliability Demonstrated','2025-12-03 19:12:27.062326','The AI system to be deployed is demonstrated to be valid (does what it is intended) and reliable. Limitations in generalizability beyond development conditions are documented.',NULL,'Validity and Reliability Evidence Pack including external validation results or shadow mode results, reliability metrics over time, limitations and generalizability statement, and documented residual risks',NULL,'MEASURE (Evaluate Trustworthiness)','Ensure the final validation phase includes demonstrating that the AI system achieves its design objectives (e.g. if it’s supposed to diagnose disease, measure diagnostic accuracy clinically). Use techniques like cross-validation, external benchmarks, or shadow mode deployment to establish reliability. Any conditions where performance is poor (e.g. model only trained on adults, so child data is out-of-scope) should be clearly documented in user and technical docs as limits. If reliability is insufficient, iterate on model or add redundancy.',5,NULL,45,5,'Full points if testing confirms the system meets its intended purpose with acceptable consistency, and any scenarios where it might not generalize (like new data types or extreme cases) are known and listed (partial if validity uncertain or limitations not assessed).',NULL,'NIST AI RMF 1.0 – Measure 2.5','AI System (Validation)','2025-12-03 19:12:27.062326'),
	 ('7c0ad81d-8c09-4a5f-8db4-7b78a5c6a20f'::uuid,'Technical','MEASURE 2.6','Regular Safety Testing','2025-12-03 19:12:27.062326','The AI system is evaluated regularly for safety risks (as identified in MAP). The deployed system is shown to be safe, residual risk is within tolerance, and it can fail safely (especially if used beyond its knowledge limits). Safety metrics include reliability, real-time monitoring, and response time to failures.',NULL,'Safety Test Protocol and Results including failure mode tests, misuse tests, safe fail behavior verification, response time to failures, residual risk acceptance record, and periodic retest schedule',NULL,'MEASURE (Evaluate Trustworthiness)','Develop a safety testing protocol for the AI (especially critical systems): e.g. simulate extreme inputs or misuse, test emergency stop or fail-safe mechanisms, and measure how quickly the system or operators respond to issues. Perform this testing at defined intervals (like before each major release or annually). Confirm that all identified safety risks are either mitigated or explicitly accepted by management as within tolerances. Any time the system’s domain is expanded (beyond original knowledge limits), redo safety tests in the new domain. Document the outcomes and adjust the system if safety metrics (like mean time to failure, error severity) are worse than acceptable.',5,NULL,46,5,'Full points if safety tests (like stress tests, failure mode testing) are conducted periodically and show the system meets safety requirements (partial if only one-time safety test or none, or if residual risk remains above tolerance without mitigation).',NULL,'NIST AI RMF 1.0 – Measure 2.6','AI System (Safety)','2025-12-03 19:12:27.062326'),
	 ('7c9d6f58-692a-481d-b098-80c99f80081c'::uuid,'Technical','MEASURE 2.7','Security & Resilience Evaluated','2025-12-03 19:12:27.062326','AI system security and resilience (as identified in MAP) are evaluated and documented.',NULL,'AI Security and Resilience Assessment Report including AI specific adversarial test results, standard security controls verification, resilience and failover tests, findings, remediation tickets, and retest evidence',NULL,'MEASURE (Evaluate Trustworthiness)','Perform security assessments focusing on AI-specific threats: for instance, attempt adversarial attacks on the model (like perturbing inputs), test for model inversion or data extraction vulnerabilities, and ensure standard IT security (authentication, encryption) is in place for AI services. Also test resilience by simulating component failures (like database down, or model returning errors) to see if the system gracefully handles it. Document all findings and fixes. Repeat these tests regularly (at least annually or upon significant changes).',5,NULL,47,5,'Full points if penetration testing or adversarial testing for the AI has been carried out and any vulnerabilities addressed, and resilience measures (e.g. fallback when component fails) tested (partial if security testing is missing or superficial).',NULL,'NIST AI RMF 1.0 – Measure 2.7','AI System (Security)','2025-12-03 19:12:27.062326'),
	 ('463c2d62-696e-4074-964e-c38851cabff7'::uuid,'Technical','MEASURE 2.9','Explainability & Interpretability Assessed','2025-12-03 19:12:27.062326','The AI model is explained, validated, and documented, and AI system output is interpreted within context – as identified in MAP – to inform responsible use and governance.',NULL,'Explainability Package including model card, interpretability analysis outputs (for example SHAP summary), domain expert review notes on sampled decisions, and end user guidance on interpreting outputs',NULL,'MEASURE (Evaluate Trustworthiness)','Use or develop model interpretability techniques (e.g. SHAP values, LIME, saliency maps) to understand key features driving the model’s predictions. Have domain experts review a sample of model decisions with these explanations to confirm they align with domain knowledge (validating that the model isn’t picking up spurious correlations). Create documentation such as a Model Card detailing model overview, intended use, limitations, and key performance results. Provide guidance to end-users on how to interpret the AI’s output correctly (for instance, confidence scores, or when to seek human judgment).',5,NULL,49,5,'Full points if the project includes creation of explainability artifacts (model cards, explanation algorithms, etc.) and validations that the model’s decisions make sense domain-wise (partial if model is a “black box” with no attempt to interpret or explain).',NULL,'NIST AI RMF 1.0 – Measure 2.9','AI System (Transparency)','2025-12-03 19:12:27.062326'),
	 ('0e5add34-fa52-4487-9f47-ae56163bb626'::uuid,'Technical','MEASURE 2.11','Fairness & Bias Evaluated','2025-12-03 19:12:27.062326','Fairness and bias issues (as identified in MAP) are evaluated and results documented.',NULL,'Fairness and Bias Evaluation Report including selected fairness metrics, segmented evaluation by relevant groups, thresholds, identified gaps, mitigation actions taken, and post mitigation retest results',NULL,'MEASURE (Evaluate Trustworthiness)','Define fairness metrics appropriate for the AI context (e.g. disparate impact ratio, equal opportunity difference) and test the model’s outputs against them using validation data segmented by group (gender, race, etc., as applicable). Document any bias found. If significant bias is detected, implement mitigation (e.g. re-sample data, adjust model or post-process outputs) and re-test. Include the bias evaluation results in the model documentation (and if required, report to stakeholders or regulators). Make this an iterative part of model development, not a one-time check.',5,NULL,51,5,'Full points if bias testing on the AI outcomes across different demographic or relevant groups is conducted and documented, and if fairness criteria are met or mitigation strategies in place (partial if bias is a concern but not evaluated or unknown).',NULL,'NIST AI RMF 1.0 – Measure 2.11','AI System (Fairness)','2025-12-03 19:12:27.062326');
